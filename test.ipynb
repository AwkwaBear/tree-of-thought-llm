{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will print true if the .env file is loaded, false if not\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backend='gpt-4', temperature=0.7, task='crosswords', task_start_index=0, task_end_index=20, naive_run=True, prompt_sample='standard', method_generate=None, method_evaluate=None, method_select='greedy', n_generate_sample=10, n_evaluate_sample=1, n_select_sample=1)\n",
      "functools.partial(<function gpt at 0x7f270d8aa170>, model='gpt-4', temperature=0.7)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/miniconda3/envs/tot/lib/python3.10/site-packages/backoff/_sync.py\", line 105, in retry\n",
      "    ret = target(*args, **kwargs)\n",
      "  File \"/home/tony/tree-of-thought-llm/src/tot/models.py\", line 20, in completions_with_backoff\n",
      "    return openai.ChatCompletion.create(**kwargs)\n",
      "  File \"/home/tony/miniconda3/envs/tot/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/tony/miniconda3/envs/tot/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/tony/miniconda3/envs/tot/lib/python3.10/site-packages/openai/api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/tony/miniconda3/envs/tot/lib/python3.10/site-packages/openai/api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/tony/miniconda3/envs/tot/lib/python3.10/site-packages/openai/api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Request too large for gpt-4 in organization org-nCmlW2CZioAY4KIMMBf8hEnI on tokens per min (TPM): Limit 10000, Requested 10531. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tony/tree-of-thought-llm/run.py\", line 69, in <module>\n",
      "    run(args)\n",
      "  File \"/home/tony/tree-of-thought-llm/run.py\", line 21, in run\n",
      "    ys, info = naive_solve(args, task, i) \n",
      "  File \"/home/tony/tree-of-thought-llm/src/tot/methods/bfs.py\", line 95, in naive_solve\n",
      "    ys = get_samples(task, x, '', args.n_generate_sample, args.prompt_sample, stop=None)\n",
      "  File \"/home/tony/tree-of-thought-llm/src/tot/methods/bfs.py\", line 46, in get_samples\n",
      "    samples = gpt(prompt, n=n_generate_sample, stop=stop)\n",
      "  File \"/home/tony/tree-of-thought-llm/src/tot/models.py\", line 24, in gpt\n",
      "    return chatgpt(messages, model=model, temperature=temperature, max_tokens=max_tokens, n=n, stop=stop)\n",
      "  File \"/home/tony/tree-of-thought-llm/src/tot/models.py\", line 32, in chatgpt\n",
      "    res = completions_with_backoff(model=model, messages=messages, temperature=temperature, max_tokens=max_tokens, n=cnt, stop=stop)\n",
      "  File \"/home/tony/miniconda3/envs/tot/lib/python3.10/site-packages/backoff/_sync.py\", line 127, in retry\n",
      "    time.sleep(seconds)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!sh scripts/crosswords/standard_sampling.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
